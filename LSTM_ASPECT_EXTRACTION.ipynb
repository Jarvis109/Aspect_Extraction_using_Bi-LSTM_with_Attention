{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import json\n","\n","# Assuming your JSON file is named 'your_file.json'\n","json_file_path = '/content/drive/MyDrive/Common files/Dataset/MAMS/train.json'\n","\n","# Read data from JSON file\n","with open(json_file_path, 'r') as file:\n","    data = json.load(file)\n","\n","# Initialize empty lists for each column\n","tokens_list = []\n","aspects_list = []\n","bio_tags_list = []\n","token_length_list = []\n","\n","# Process each entry in the data\n","for entry in data:\n","    tokens = entry[\"token\"]\n","    aspects = entry.get(\"aspects\", [])\n","\n","    # Extract unique aspects as a list of terms\n","    unique_aspects = list(set(term for aspect in aspects for term in aspect.get(\"term\", [])))\n","\n","    # Initialize BIO tags list with 'O' (outside) for each token\n","    bio_tags = ['O'] * len(tokens)\n","\n","    # Process aspects and update BIO tags accordingly\n","    for aspect in aspects:\n","        term = aspect.get(\"term\", [])\n","        for i in range(len(tokens)):\n","            if tokens[i:i + len(term)] == term:\n","                if i > 0 and bio_tags[i - 1] == 'B':\n","                    bio_tags[i] = 'I'\n","                else:\n","                    bio_tags[i] = 'B'\n","                if len(term) > 1:\n","                    bio_tags[i + 1:i + len(term)] = ['I'] * (len(term) - 1)\n","\n","    # Append data to lists\n","    tokens_list.append(tokens)\n","    aspects_list.append(unique_aspects)  # Append unique aspects\n","    bio_tags_list.append(bio_tags)\n","    token_length_list.append([len(token) for token in tokens])\n","\n","# Create DataFrame\n","df_result = pd.DataFrame({\n","    'token': tokens_list,\n","    'aspect': aspects_list,\n","    'bioTag': bio_tags_list,\n","    'token_length': token_length_list\n","})\n","\n","# Save DataFrame to CSV file\n","csv_output_path = 'output_file.csv'\n","df_result.to_csv(csv_output_path, index=False)\n","\n","# Display the DataFrame\n","print(df_result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rVM55YLZkutW","executionInfo":{"status":"ok","timestamp":1709473754783,"user_tz":-330,"elapsed":1216,"user":{"displayName":"ALOK RAWAT","userId":"04725056840952316596"}},"outputId":"ffb55b90-52c1-4c15-c5f6-802224e34023"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                  token  \\\n","0     [the, decor, is, not, special, at, all, but, t...   \n","1     [when, tables, opened, up, ,, the, manager, sa...   \n","2     [though, the, menu, includes, some, unorthodox...   \n","3     [service, is, good, although, a, bit, in, your...   \n","4     [ps-, i, just, went, for, brunch, on, saturday...   \n","...                                                 ...   \n","4292  [for, dinner, ,, i, love, the, churrasco, and,...   \n","4293  [was, there, for, dinner, last, night, ,, and,...   \n","4294  [the, menu, sounded, good, but, the, grilled, ...   \n","4295  [service, is, coddling, and, correct, and, the...   \n","4296  [usc, has, a, cold, smoker, and, smoked, the, ...   \n","\n","                                                 aspect  \\\n","0                                 [prices, decor, food]   \n","1                                     [manager, tables]   \n","2         [classics, butter, peanut, roll, sushi, menu]   \n","3                                       [service, food]   \n","4                  [eggs, served, onions, with, brunch]   \n","...                                                 ...   \n","4292  [of, churrasco, black, beans, dinner, with, ha...   \n","4293                                     [dinner, food]   \n","4294                    [eggplant, roll, grilled, menu]   \n","4295                                  [service, cheese]   \n","4296                                    [dish, avocado]   \n","\n","                                                 bioTag  \\\n","0     [O, B, O, O, O, O, O, O, O, B, O, O, B, O, O, ...   \n","1               [O, B, O, O, O, O, B, O, O, O, O, O, O]   \n","2     [O, O, B, O, O, O, O, O, O, B, I, I, O, O, O, ...   \n","3     [B, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n","4     [O, O, O, O, O, B, O, O, O, O, B, I, I, I, O, ...   \n","...                                                 ...   \n","4292  [O, B, O, O, O, O, B, O, B, I, I, I, I, I, O, ...   \n","4293            [O, O, O, B, O, O, O, O, O, B, O, O, O]   \n","4294  [O, B, O, O, O, O, B, I, I, O, O, O, O, O, O, ...   \n","4295  [B, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n","4296  [O, O, O, O, O, O, O, O, B, O, O, O, O, O, B, ...   \n","\n","                                           token_length  \n","0     [3, 5, 2, 3, 7, 2, 3, 3, 5, 4, 3, 7, 6, 4, 2, ...  \n","1               [4, 6, 6, 2, 1, 3, 7, 3, 7, 5, 6, 2, 1]  \n","2     [6, 3, 4, 8, 4, 10, 9, 1, 1, 6, 6, 4, 1, 3, 8,...  \n","3     [7, 2, 4, 8, 1, 3, 2, 4, 4, 1, 2, 4, 5, 5, 4, ...  \n","4     [3, 1, 4, 4, 3, 6, 2, 8, 3, 3, 4, 6, 4, 6, 3, ...  \n","...                                                 ...  \n","4292  [3, 6, 1, 1, 4, 3, 9, 3, 7, 4, 2, 6, 5, 5, 3, ...  \n","4293            [3, 5, 3, 6, 4, 5, 1, 3, 3, 4, 3, 5, 1]  \n","4294  [3, 4, 7, 4, 3, 3, 7, 8, 4, 3, 10, 4, 1, 4, 2,...  \n","4295  [7, 2, 8, 3, 7, 3, 5, 2, 2, 5, 2, 5, 1, 2, 6, ...  \n","4296  [3, 3, 1, 4, 6, 3, 6, 3, 7, 2, 5, 2, 4, 3, 4, ...  \n","\n","[4297 rows x 4 columns]\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical"],"metadata":{"id":"1ygTeu6xmAj-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import json\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout\n","from keras.preprocessing.text import Tokenizer\n","\n","# Load the data from JSON file\n","json_file_path = '/content/drive/MyDrive/Common files/Dataset/MAMS/train.json'\n","with open(json_file_path, 'r') as file:\n","    data = json.load(file)\n","\n","# Initialize empty lists for each column\n","tokens_list = []\n","aspects_list = []\n","bio_tags_list = []\n","token_length_list = []\n","\n","# Process each entry in the data\n","for entry in data:\n","    tokens = entry[\"token\"]\n","    aspects = entry.get(\"aspects\", [])\n","\n","    # Extract unique aspects as a list of terms\n","    unique_aspects = list(set(term for aspect in aspects for term in aspect.get(\"term\", [])))\n","\n","    # Initialize BIO tags list with 'O' (outside) for each token\n","    bio_tags = ['O'] * len(tokens)\n","\n","    # Process aspects and update BIO tags accordingly\n","    for aspect in aspects:\n","        term = aspect.get(\"term\", [])\n","        for i in range(len(tokens)):\n","            if tokens[i:i + len(term)] == term:\n","                if i > 0 and bio_tags[i - 1] == 'B':\n","                    bio_tags[i] = 'I'\n","                else:\n","                    bio_tags[i] = 'B'\n","                if len(term) > 1:\n","                    bio_tags[i + 1:i + len(term)] = ['I'] * (len(term) - 1)\n","\n","    # Append data to lists\n","    tokens_list.append(tokens)\n","    aspects_list.append(unique_aspects)  # Append unique aspects\n","    bio_tags_list.append(bio_tags)\n","    token_length_list.append([len(token) for token in tokens])\n","\n","# Create DataFrame\n","df_result = pd.DataFrame({\n","    'token': tokens_list,\n","    'aspect': aspects_list,\n","    'bioTag': bio_tags_list,\n","    'token_length': token_length_list\n","})\n","\n","# Create a tokenizer and fit on the tokenized sentences\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(df_result['token'])\n","\n","# Convert tokens to numerical representations\n","X = tokenizer.texts_to_sequences(df_result['token'])\n","\n","# Pad sequences to ensure uniform length\n","max_seq_length = max(len(seq) for seq in X)\n","X = pad_sequences(X, maxlen=max_seq_length, padding='post')\n","\n","# Create a dictionary to map BIO tags to numerical representations\n","tag_to_index = {'O': 0, 'B': 1, 'I': 2}\n","\n","# Convert BIO tags to numerical representations using the dictionary\n","y = [[tag_to_index[tag] for tag in seq] for seq in df_result['bioTag']]\n","\n","# Pad sequences to ensure uniform length\n","y = pad_sequences(y, padding='post', value=-1)\n","\n","# Convert numerical representations to one-hot encoding\n","y = to_categorical(y)\n","\n","# Split the data into training, validation, and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n","\n","# Define the LSTM model\n","model = Sequential()\n","model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=X_train.shape[1]))\n","model.add(Dropout(0.1))\n","model.add(LSTM(units=100, return_sequences=True))\n","model.add(TimeDistributed(Dense(y.shape[2], activation='softmax')))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=32, epochs=10)\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(X_test, y_test)\n","print(\"Test Loss:\", loss)\n","print(\"Test Accuracy:\", accuracy)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3CTosWvrmEqq","executionInfo":{"status":"ok","timestamp":1709474495624,"user_tz":-330,"elapsed":146708,"user":{"displayName":"ALOK RAWAT","userId":"04725056840952316596"}},"outputId":"f7eb3151-2446-47ac-be1b-34de07eab201"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","97/97 [==============================] - 12s 95ms/step - loss: 0.3335 - accuracy: 0.8933 - val_loss: 0.1752 - val_accuracy: 0.9490\n","Epoch 2/10\n","97/97 [==============================] - 8s 80ms/step - loss: 0.1409 - accuracy: 0.9513 - val_loss: 0.1008 - val_accuracy: 0.9599\n","Epoch 3/10\n","97/97 [==============================] - 9s 94ms/step - loss: 0.0698 - accuracy: 0.9736 - val_loss: 0.0768 - val_accuracy: 0.9721\n","Epoch 4/10\n","97/97 [==============================] - 7s 76ms/step - loss: 0.0530 - accuracy: 0.9795 - val_loss: 0.0754 - val_accuracy: 0.9729\n","Epoch 5/10\n","97/97 [==============================] - 9s 93ms/step - loss: 0.0459 - accuracy: 0.9823 - val_loss: 0.0746 - val_accuracy: 0.9730\n","Epoch 6/10\n","97/97 [==============================] - 8s 81ms/step - loss: 0.0417 - accuracy: 0.9839 - val_loss: 0.0776 - val_accuracy: 0.9731\n","Epoch 7/10\n","97/97 [==============================] - 8s 87ms/step - loss: 0.0383 - accuracy: 0.9853 - val_loss: 0.0811 - val_accuracy: 0.9721\n","Epoch 8/10\n","97/97 [==============================] - 9s 92ms/step - loss: 0.0348 - accuracy: 0.9868 - val_loss: 0.0863 - val_accuracy: 0.9697\n","Epoch 9/10\n","97/97 [==============================] - 7s 76ms/step - loss: 0.0320 - accuracy: 0.9880 - val_loss: 0.0872 - val_accuracy: 0.9711\n","Epoch 10/10\n","97/97 [==============================] - 9s 93ms/step - loss: 0.0290 - accuracy: 0.9893 - val_loss: 0.0902 - val_accuracy: 0.9706\n","27/27 [==============================] - 1s 31ms/step - loss: 0.0802 - accuracy: 0.9738\n","Test Loss: 0.08019061386585236\n","Test Accuracy: 0.9738039970397949\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"PP8Em3XSsudL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sample test sentences\n","test_sentences = [\n","    \"The food was delicious but the service was slow.\",\n","    \"I loved the ambiance of the restaurant.\",\n","    \"The prices were too high for the quality of food.\",\n","    \"The staff was friendly and helpful.\",\n","    \"The phone battery is good.\"\n","]\n","\n","# Tokenize test sentences\n","tokenized_test_sentences = tokenizer.texts_to_sequences(test_sentences)\n","\n","# Pad sequences to ensure uniform length\n","tokenized_test_sentences = pad_sequences(tokenized_test_sentences, maxlen=max_seq_length, padding='post')\n","\n","# Predict BIO tags for test sentences\n","predictions = model.predict(tokenized_test_sentences)\n","\n","# Convert predictions to BIO tags\n","predicted_tags = []\n","for pred in predictions:\n","    pred_tags = [np.argmax(tag) for tag in pred]\n","    predicted_tags.append(pred_tags)\n","\n","# Convert numerical representations to BIO tags\n","index_to_tag = {index: tag for tag, index in tag_to_index.items()}\n","predicted_tags = [[index_to_tag[index] for index in seq] for seq in predicted_tags]\n","\n","def extract_aspects(sentence, bio_tags):\n","    aspects = []\n","    current_aspect = \"\"\n","    for i, tag in enumerate(bio_tags):\n","        if i < len(sentence):  # Check if the index is within the range of the sentence\n","            if tag == \"B\":\n","                if current_aspect:\n","                    aspects.append(current_aspect)\n","                current_aspect = sentence[i]\n","            elif tag == \"I\":\n","                current_aspect += \" \" + sentence[i]\n","    if current_aspect:\n","        aspects.append(current_aspect)\n","    return aspects\n","\n","# Display the predicted aspects for each test sentence\n","for sentence, tags in zip(test_sentences, predicted_tags):\n","    aspects = extract_aspects(sentence.split(), tags)\n","    print(\"Sentence:\", sentence)\n","    print(\"Predicted Aspects:\", aspects)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3CKOrUXNopwy","executionInfo":{"status":"ok","timestamp":1709475769258,"user_tz":-330,"elapsed":482,"user":{"displayName":"ALOK RAWAT","userId":"04725056840952316596"}},"outputId":"c3d7ed42-1770-49ae-ed6b-f84cd57f0cbc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 46ms/step\n","Sentence: The food was delicious but the service was slow.\n","Predicted Aspects: ['food', 'service']\n","\n","Sentence: I loved the ambiance of the restaurant.\n","Predicted Aspects: ['ambiance']\n","\n","Sentence: The prices were too high for the quality of food.\n","Predicted Aspects: ['prices', 'quality', 'food.']\n","\n","Sentence: The staff was friendly and helpful.\n","Predicted Aspects: ['staff']\n","\n","Sentence: The phone battery is good.\n","Predicted Aspects: ['good.']\n","\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","\n","# Predict BIO tags for test sentences\n","predictions = model.predict(X_test)\n","\n","# Convert predictions to BIO tags\n","predicted_tags = []\n","for pred in predictions:\n","    pred_tags = [np.argmax(tag) for tag in pred]\n","    predicted_tags.append(pred_tags)\n","\n","# Flatten the true and predicted BIO tags\n","y_true = np.argmax(y_test, axis=2).flatten()\n","y_pred = np.concatenate(predicted_tags)\n","\n","# Generate confusion matrix\n","conf_matrix = confusion_matrix(y_true, y_pred)\n","\n","# Display the confusion matrix\n","print(\"Confusion Matrix:\")\n","print(conf_matrix)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UOigmMRdpMs0","executionInfo":{"status":"ok","timestamp":1709475662341,"user_tz":-330,"elapsed":1223,"user":{"displayName":"ALOK RAWAT","userId":"04725056840952316596"}},"outputId":"ced51e17-03ca-4652-965c-d984d13c3f6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["27/27 [==============================] - 1s 19ms/step\n","Confusion Matrix:\n","[[19036   500   261]\n"," [  444  1683    77]\n"," [  215    80 37904]]\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import json\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout\n","from keras.preprocessing.text import Tokenizer\n","\n","# Load the data from JSON file\n","json_file_path = '/content/drive/MyDrive/Common files/Dataset/Laptops/train.json'\n","with open(json_file_path, 'r') as file:\n","    data = json.load(file)\n","\n","# Initialize empty lists for each column\n","tokens_list = []\n","aspects_list = []\n","bio_tags_list = []\n","token_length_list = []\n","\n","# Process each entry in the data\n","for entry in data:\n","    tokens = entry[\"token\"]\n","    aspects = entry.get(\"aspects\", [])\n","\n","    # Extract unique aspects as a list of terms\n","    unique_aspects = list(set(term for aspect in aspects for term in aspect.get(\"term\", [])))\n","\n","    # Initialize BIO tags list with 'O' (outside) for each token\n","    bio_tags = ['O'] * len(tokens)\n","\n","    # Process aspects and update BIO tags accordingly\n","    for aspect in aspects:\n","        term = aspect.get(\"term\", [])\n","        for i in range(len(tokens)):\n","            if tokens[i:i + len(term)] == term:\n","                if i > 0 and bio_tags[i - 1] == 'B':\n","                    bio_tags[i] = 'I'\n","                else:\n","                    bio_tags[i] = 'B'\n","                if len(term) > 1:\n","                    bio_tags[i + 1:i + len(term)] = ['I'] * (len(term) - 1)\n","\n","    # Append data to lists\n","    tokens_list.append(tokens)\n","    aspects_list.append(unique_aspects)  # Append unique aspects\n","    bio_tags_list.append(bio_tags)\n","    token_length_list.append([len(token) for token in tokens])\n","\n","# Create DataFrame\n","df_result = pd.DataFrame({\n","    'token': tokens_list,\n","    'aspect': aspects_list,\n","    'bioTag': bio_tags_list,\n","    'token_length': token_length_list\n","})\n","\n","# Create a tokenizer and fit on the tokenized sentences\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(df_result['token'])\n","\n","# Convert tokens to numerical representations\n","X = tokenizer.texts_to_sequences(df_result['token'])\n","\n","# Pad sequences to ensure uniform length\n","max_seq_length = max(len(seq) for seq in X)\n","X = pad_sequences(X, maxlen=max_seq_length, padding='post')\n","\n","# Create a dictionary to map BIO tags to numerical representations\n","tag_to_index = {'O': 0, 'B': 1, 'I': 2}\n","\n","# Convert BIO tags to numerical representations using the dictionary\n","y = [[tag_to_index[tag] for tag in seq] for seq in df_result['bioTag']]\n","\n","# Pad sequences to ensure uniform length\n","y = pad_sequences(y, padding='post', value=-1)\n","\n","# Convert numerical representations to one-hot encoding\n","y = to_categorical(y)\n","\n","# Split the data into training, validation, and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n","\n","# Define the LSTM model\n","model = Sequential()\n","model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=X_train.shape[1]))\n","model.add(Dropout(0.1))\n","model.add(LSTM(units=100, return_sequences=True))\n","model.add(TimeDistributed(Dense(y.shape[2], activation='softmax')))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=32, epochs=10)\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(X_test, y_test)\n","print(\"Test Loss:\", loss)\n","print(\"Test Accuracy:\", accuracy)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BElqdmcJsvzK","executionInfo":{"status":"ok","timestamp":1709475909458,"user_tz":-330,"elapsed":45480,"user":{"displayName":"ALOK RAWAT","userId":"04725056840952316596"}},"outputId":"20869bd9-b0e1-4615-9700-2df65b370505"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","33/33 [==============================] - 6s 104ms/step - loss: 0.4912 - accuracy: 0.8182 - val_loss: 0.2584 - val_accuracy: 0.9599\n","Epoch 2/10\n","33/33 [==============================] - 3s 84ms/step - loss: 0.1931 - accuracy: 0.9631 - val_loss: 0.1459 - val_accuracy: 0.9620\n","Epoch 3/10\n","33/33 [==============================] - 3s 84ms/step - loss: 0.1269 - accuracy: 0.9671 - val_loss: 0.1214 - val_accuracy: 0.9663\n","Epoch 4/10\n","33/33 [==============================] - 3s 101ms/step - loss: 0.1082 - accuracy: 0.9680 - val_loss: 0.1060 - val_accuracy: 0.9670\n","Epoch 5/10\n","33/33 [==============================] - 4s 114ms/step - loss: 0.0909 - accuracy: 0.9695 - val_loss: 0.0891 - val_accuracy: 0.9691\n","Epoch 6/10\n","33/33 [==============================] - 3s 84ms/step - loss: 0.0709 - accuracy: 0.9726 - val_loss: 0.0706 - val_accuracy: 0.9731\n","Epoch 7/10\n","33/33 [==============================] - 3s 81ms/step - loss: 0.0506 - accuracy: 0.9820 - val_loss: 0.0570 - val_accuracy: 0.9796\n","Epoch 8/10\n","33/33 [==============================] - 3s 83ms/step - loss: 0.0383 - accuracy: 0.9881 - val_loss: 0.0517 - val_accuracy: 0.9831\n","Epoch 9/10\n","33/33 [==============================] - 4s 118ms/step - loss: 0.0316 - accuracy: 0.9905 - val_loss: 0.0498 - val_accuracy: 0.9839\n","Epoch 10/10\n","33/33 [==============================] - 3s 96ms/step - loss: 0.0271 - accuracy: 0.9916 - val_loss: 0.0494 - val_accuracy: 0.9838\n","10/10 [==============================] - 0s 35ms/step - loss: 0.0511 - accuracy: 0.9847\n","Test Loss: 0.051133111119270325\n","Test Accuracy: 0.984722375869751\n"]}]}]}